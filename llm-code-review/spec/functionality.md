# LLM Code Review - Guided Review Checklist

## Purpose

A structured review checklist for code generated by LLMs. Instead of automated analysis, it guides the human reviewer through the failure modes most common in LLM-generated code, ensuring nothing important gets skipped.

The core insight: LLM code fails differently than human code. It looks correct at a glance, compiles, and often runs - but has subtle issues that require a different review lens. This tool provides that lens.

## Core Workflow

1. **Input** - Reviewer pastes two things side by side:
   - The **prompt/spec** (what was asked for)
   - The **generated code** (what was produced)

2. **Review** - Walk through a checklist of review categories, each with specific items to verify. Items are ordered by impact, not difficulty.

3. **Annotate** - For each item, mark it as:
   - **Pass** - Verified, no issues
   - **Flag** - Issue found (with optional note)
   - **Skip** - Not applicable to this code

4. **Summary** - View a review summary with all flagged items, notes, and an overall assessment.

## Review Categories

The checklist is organized into categories, presented in order of descending value:

### 1. Spec Fidelity
Does the code actually do what was asked?

- [ ] Every requirement in the prompt has corresponding code
- [ ] No features were added that weren't requested
- [ ] Edge cases mentioned in the prompt are handled
- [ ] The code doesn't subtly reinterpret or narrow the requirements

### 2. Hallucinated APIs
Does the code reference things that actually exist?

- [ ] All imports/dependencies are real and resolve
- [ ] API methods and function signatures match real documentation
- [ ] CSS properties and HTML attributes are valid
- [ ] Browser APIs used exist and have adequate support

### 3. Subtle Wrongness
Does the logic actually do what it looks like it does?

- [ ] Boundary conditions are correct (off-by-one, <= vs <)
- [ ] Comparison operators are correct (== vs ===, negation logic)
- [ ] Arguments are in the correct order
- [ ] Return values are used/checked correctly
- [ ] Async/await and promise handling is correct

### 4. Security Surface
Are there common security mistakes?

- [ ] No innerHTML with user-controlled content (use textContent)
- [ ] Input is validated at system boundaries
- [ ] No hardcoded secrets or credentials
- [ ] URLs and paths are properly sanitized
- [ ] No eval() or equivalent dynamic code execution

### 5. Unnecessary Complexity
Did the LLM over-engineer?

- [ ] No abstractions that are only used once
- [ ] No error handling for impossible cases
- [ ] No configuration for things that don't vary
- [ ] No premature optimization
- [ ] Code is as simple as it could be while meeting requirements

### 6. Dead Code
Are there generation artifacts?

- [ ] No unused imports or variables
- [ ] No unreachable branches or conditions
- [ ] No commented-out code
- [ ] No placeholder or TODO code left behind
- [ ] No duplicate logic that could be a single function

### 7. Codebase Consistency
Does it fit the surrounding project?

- [ ] Naming conventions match the existing code
- [ ] Error handling follows the same pattern
- [ ] File/folder structure follows project conventions
- [ ] State management approach is consistent

## Features

### Side-by-Side Input
- Two text areas: one for prompt/spec, one for generated code
- Both support plain text paste
- Resizable panes
- Collapsible after pasting, to give more room to the checklist

### Checklist Interface
- Categories displayed as expandable sections
- Each item has Pass / Flag / Skip controls
- Flag action expands a text input for a note
- Progress indicator showing how many items reviewed vs total
- Category-level summary (e.g., "3/4 passed, 1 flagged")

### Review Summary
- Generated after completing the checklist (or on demand)
- Lists all flagged items grouped by category
- Includes reviewer notes
- Shows pass/flag/skip counts
- Copyable as plain text (markdown format) for pasting into PRs or docs

### Persistence
- Auto-save current review to localStorage
- Ability to start a fresh review (with confirmation if current has progress)
- No server-side storage needed

### Keyboard Navigation
- Tab through checklist items
- Keyboard shortcuts for Pass (p), Flag (f), Skip (s)
- Enter to expand/collapse categories

## UI Layout

```
+------------------------------------------+
|  LLM Code Review                         |
+------------------------------------------+
|  [Prompt/Spec]    |  [Generated Code]    |
|                   |                      |
|  (text area)      |  (text area)         |
|                   |                      |
+------------------------------------------+
|  Review Checklist                        |
|                                          |
|  > 1. Spec Fidelity         [2/4] ----  |
|  > 2. Hallucinated APIs     [0/4] ----  |
|  > 3. Subtle Wrongness      [0/5]       |
|  > 4. Security Surface      [0/5]       |
|  > 5. Unnecessary Complexity [0/5]      |
|  > 6. Dead Code             [0/5]       |
|  > 7. Codebase Consistency  [0/4]       |
|                                          |
|  [Generate Summary]                      |
+------------------------------------------+
```

## Technical Implementation

### Technology Stack
- Vanilla JavaScript, CSS, HTML
- No external dependencies
- localStorage for persistence

### File Organization
```
llm-code-review/
  index.html
  style.css
  script.js
  spec/
    functionality.md
  sessions/
```

### Design
- Grayscale minimalist, consistent with other apps in this repo
- Compact layout - the checklist is the focus, not the text areas
- Text areas collapsible after pasting input

## Limitations

- No actual code analysis - this is a human-guided tool
- No syntax highlighting (keeping it dependency-free)
- Single review at a time (localStorage holds one active review)
- No export to file, only copy-to-clipboard

## Use Cases

- Reviewing code generated by Claude, ChatGPT, Copilot, or other LLMs
- Training yourself to catch LLM-specific failure modes
- Producing structured review notes for PRs that include AI-generated code
- Building review discipline when working with AI assistants
